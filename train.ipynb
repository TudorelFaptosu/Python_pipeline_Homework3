{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPrdWUYFPOK8Zym+88OOLlh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TudorelFaptosu/Python_pipeline_Homework3/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzfO4DX5cHfL",
        "outputId": "62d121b1-1809-4b96-da0d-4cbd1e04f0fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train.py\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# --- 1. Custom Optimizers ---\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.defaults.update(self.base_optimizer.defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]\n",
        "        self.base_optimizer.step()\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device\n",
        "        norm = torch.norm(\n",
        "            torch.stack([\n",
        "                ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                for group in self.param_groups for p in group[\"params\"] if p.grad is not None\n",
        "            ]), p=2\n",
        "        )\n",
        "        return norm\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        raise NotImplementedError(\"SAM requires closure, use first_step and second_step\")\n",
        "\n",
        "class Muon(optim.SGD):\n",
        "    def __init__(self, params, lr=0.02, momentum=0.95, weight_decay=1e-4):\n",
        "        super().__init__(params, lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=True)\n",
        "\n",
        "# --- 2. Setup ---\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--dataset', type=str, default='CIFAR100', choices=['MNIST', 'CIFAR10', 'CIFAR100', 'OxfordIIITPet'])\n",
        "    parser.add_argument('--data_dir', type=str, default='./data')\n",
        "    parser.add_argument('--model', type=str, default='resnest26d', choices=['resnet18', 'resnet50', 'resnest14d', 'resnest26d', 'mlp'])\n",
        "    parser.add_argument('--pretrained', action='store_true')\n",
        "    parser.add_argument('--num_classes', type=int, default=100)\n",
        "    parser.add_argument('--epochs', type=int, default=50)\n",
        "    parser.add_argument('--batch_size', type=int, default=128)\n",
        "    parser.add_argument('--image_size', type=int, default=224)\n",
        "    parser.add_argument('--lr', type=float, default=0.001)\n",
        "    parser.add_argument('--optimizer', type=str, default='AdamW', choices=['SGD', 'Adam', 'AdamW', 'Muon', 'SAM'])\n",
        "    parser.add_argument('--scheduler', type=str, default='ReduceLROnPlateau', choices=['StepLR', 'ReduceLROnPlateau'])\n",
        "    parser.add_argument('--patience', type=int, default=5)\n",
        "    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    parser.add_argument('--num_workers', type=int, default=2) # Colab usually prefers 2 workers\n",
        "    parser.add_argument('--exp_name', type=str, default='experiment_1')\n",
        "    parser.add_argument('--batch_schedule', type=str, default=None)\n",
        "    return parser.parse_args()\n",
        "\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# --- 3. Components ---\n",
        "def get_transforms(args, is_train=True):\n",
        "    mean, std = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "    if args.dataset == 'MNIST': mean, std = (0.1307,), (0.3081,)\n",
        "    elif args.dataset in ['CIFAR100', 'CIFAR10']: mean, std = (0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    ts = [transforms.Resize((args.image_size, args.image_size))]\n",
        "    if is_train:\n",
        "        ts.append(transforms.RandomHorizontalFlip())\n",
        "        if args.dataset != 'MNIST':\n",
        "            ts.append(transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10))\n",
        "            ts.append(transforms.RandomCrop(args.image_size, padding=4))\n",
        "        ts.append(transforms.ToTensor())\n",
        "        ts.append(transforms.Normalize(mean, std))\n",
        "        ts.append(transforms.RandomErasing(p=0.25))\n",
        "    else:\n",
        "        ts.append(transforms.ToTensor())\n",
        "        ts.append(transforms.Normalize(mean, std))\n",
        "    return transforms.Compose(ts)\n",
        "\n",
        "def get_dataset(args):\n",
        "    # This automatically downloads data to ./data folder\n",
        "    train_transform = get_transforms(args, is_train=True)\n",
        "    test_transform = get_transforms(args, is_train=False)\n",
        "\n",
        "    if args.dataset == 'MNIST':\n",
        "        train_ds = datasets.MNIST(args.data_dir, train=True, download=True, transform=train_transform)\n",
        "        test_ds = datasets.MNIST(args.data_dir, train=False, download=True, transform=test_transform)\n",
        "        args.num_classes = 10\n",
        "    elif args.dataset == 'CIFAR10':\n",
        "        train_ds = datasets.CIFAR10(args.data_dir, train=True, download=True, transform=train_transform)\n",
        "        test_ds = datasets.CIFAR10(args.data_dir, train=False, download=True, transform=test_transform)\n",
        "        args.num_classes = 10\n",
        "    elif args.dataset == 'CIFAR100':\n",
        "        train_ds = datasets.CIFAR100(args.data_dir, train=True, download=True, transform=train_transform)\n",
        "        test_ds = datasets.CIFAR100(args.data_dir, train=False, download=True, transform=test_transform)\n",
        "        args.num_classes = 100\n",
        "    elif args.dataset == 'OxfordIIITPet':\n",
        "        train_ds = datasets.OxfordIIITPet(args.data_dir, split='trainval', download=True, transform=train_transform)\n",
        "        test_ds = datasets.OxfordIIITPet(args.data_dir, split='test', download=True, transform=test_transform)\n",
        "        args.num_classes = 37\n",
        "    return train_ds, test_ds\n",
        "\n",
        "class MLPWrapper(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "    def forward(self, x): return self.net(self.flatten(x))\n",
        "\n",
        "def get_model(args):\n",
        "    if args.model == 'mlp':\n",
        "        input_channels = 1 if args.dataset == 'MNIST' else 3\n",
        "        input_dim = input_channels * args.image_size * args.image_size\n",
        "        model = MLPWrapper(input_dim, 512, args.num_classes)\n",
        "    else:\n",
        "        model = timm.create_model(args.model, pretrained=args.pretrained, num_classes=args.num_classes, in_chans=1 if args.dataset=='MNIST' else 3)\n",
        "    return model.to(args.device)\n",
        "\n",
        "def get_optimizer(model, args):\n",
        "    if args.optimizer == 'SGD': return optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n",
        "    elif args.optimizer == 'Adam': return optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
        "    elif args.optimizer == 'AdamW': return optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.05)\n",
        "    elif args.optimizer == 'Muon': return Muon(model.parameters(), lr=args.lr)\n",
        "    elif args.optimizer == 'SAM':\n",
        "        base_optim = optim.AdamW if args.pretrained else optim.SGD\n",
        "        return SAM(model.parameters(), base_optim, lr=args.lr, momentum=0.9)\n",
        "    return optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "class BatchSizeScheduler:\n",
        "    def __init__(self, schedule_str):\n",
        "        self.schedule = {}\n",
        "        if schedule_str:\n",
        "            for part in schedule_str.split(','):\n",
        "                ep, sz = part.split(':')\n",
        "                self.schedule[int(ep)] = int(sz)\n",
        "    def get_batch_size(self, epoch, current_size):\n",
        "        if epoch in self.schedule:\n",
        "            print(f\"Batch Scheduler: Changing batch size to {self.schedule[epoch]}\")\n",
        "            return self.schedule[epoch]\n",
        "        return current_size\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience; self.min_delta = min_delta; self.counter = 0; self.best_loss = None; self.early_stop = False\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None: self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience: self.early_stop = True\n",
        "        else: self.best_loss = val_loss; self.counter = 0\n",
        "\n",
        "# --- 4. Main ---\n",
        "def main():\n",
        "    args = get_args()\n",
        "    seed_everything()\n",
        "    log_dir = os.path.join(\"runs\", args.exp_name)\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "    train_ds, test_ds = get_dataset(args)\n",
        "    current_batch_size = args.batch_size\n",
        "    train_loader = DataLoader(train_ds, batch_size=current_batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, persistent_workers=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=current_batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
        "\n",
        "    model = get_model(args)\n",
        "    optimizer = get_optimizer(model, args)\n",
        "    scheduler = get_scheduler(optimizer, args) if args.scheduler else None\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    batch_scheduler = BatchSizeScheduler(args.batch_schedule)\n",
        "    early_stopping = EarlyStopping(patience=args.patience)\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    print(f\"Starting {args.exp_name} on {args.device} with {args.model}...\")\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        new_bs = batch_scheduler.get_batch_size(epoch, current_batch_size)\n",
        "        if new_bs != current_batch_size:\n",
        "            current_batch_size = new_bs\n",
        "            train_loader = DataLoader(train_ds, batch_size=current_batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "        # Train\n",
        "        model.train()\n",
        "        r_loss, correct, total = 0.0, 0, 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False)\n",
        "        for imgs, lbls in pbar:\n",
        "            imgs, lbls = imgs.to(args.device), lbls.to(args.device)\n",
        "            if args.optimizer == 'SAM':\n",
        "                with autocast(): criterion(model(imgs), lbls).backward()\n",
        "                optimizer.first_step(zero_grad=True)\n",
        "                with autocast(): criterion(model(imgs), lbls).backward()\n",
        "                optimizer.second_step(zero_grad=True)\n",
        "            else:\n",
        "                optimizer.zero_grad()\n",
        "                with autocast():\n",
        "                    out = model(imgs)\n",
        "                    loss = criterion(out, lbls)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                r_loss += loss.item()\n",
        "                _, pred = out.max(1)\n",
        "                total += lbls.size(0)\n",
        "                correct += pred.eq(lbls).sum().item()\n",
        "                pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        train_loss, train_acc = r_loss/len(train_loader), 100.*correct/total\n",
        "\n",
        "        # Eval\n",
        "        model.eval()\n",
        "        r_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, lbls in test_loader:\n",
        "                imgs, lbls = imgs.to(args.device), lbls.to(args.device)\n",
        "                out = model(imgs)\n",
        "                r_loss += criterion(out, lbls).item()\n",
        "                _, pred = out.max(1)\n",
        "                total += lbls.size(0)\n",
        "                correct += pred.eq(lbls).sum().item()\n",
        "        val_loss, val_acc = r_loss/len(test_loader), 100.*correct/total\n",
        "\n",
        "        # Update Scheduler\n",
        "        if args.scheduler == 'ReduceLROnPlateau' and scheduler: scheduler.step(val_loss)\n",
        "        elif args.scheduler == 'StepLR' and scheduler: scheduler.step()\n",
        "\n",
        "        print(f\"Ep {epoch+1} | TrainAcc: {train_acc:.2f}% | ValAcc: {val_acc:.2f}%\")\n",
        "        writer.add_scalar('Accuracy/Val', val_acc, epoch)\n",
        "        writer.add_scalar('Loss/Val', val_loss, epoch)\n",
        "\n",
        "        if val_acc > best_acc: best_acc = val_acc\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop: break\n",
        "\n",
        "    print(f\"Best Acc: {best_acc:.2f}%\")\n",
        "    writer.close()\n",
        "\n",
        "def get_scheduler(optimizer, args):\n",
        "    opt = optimizer.base_optimizer if args.optimizer == 'SAM' else optimizer\n",
        "    if args.scheduler == 'StepLR': return StepLR(opt, step_size=10, gamma=0.1)\n",
        "    elif args.scheduler == 'ReduceLROnPlateau': return ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=3)\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ztivh3pcQEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}